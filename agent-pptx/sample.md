# 高效前缀缓存加速 LLM 多 Agent 工作流，实现最高 2.19×速度提升 🚀（38 字内）

**名称**：KVFlow: Efficient Prefix Caching for Accelerating LLM‑Based Multi‑Agent Workflows **作者**：Zaifeng Pan, Ajjkumar Patel, Zhengding Hu, Yipeng Shen, Yue Guan, Wan‑Lu Li, Lianhui Qin, Yida Wang, Yufei Ding.

## 摘要

- 本文聚焦于基于大语言模型（LLM）的多 Agent 协作工作流，其中每个 Agent 使用固定提示（prompt），可复用其对应的密钥‑值（KV）张量以避免重复推理计算。
- 当前系统一般采用 LRU 淘汰策略管理 KV 缓存，缺乏对未来调用的预测，导致 KV 缓存常在重用前被销毁，引发缓存未命中、重复计算及内存交换开销。
- 提出 **KVFlow**：一种面向工作流的 KV 缓存管理框架，结合预取机制，在后台线程提前加载下一步 Agent 所需的 KV 张量，从 CPU 移动至 GPU，实现全覆盖的预取，避免生成期间的缓存未命中停顿。
- 实验表明，在单个包含大提示的工作流中，KVFlow 相较于 SGLang 加层级基数缓存结构可达 **1.83×** 加速，多并发工作流场景下最高达 **2.19×** 加速，显著提升推理效率.

## 关键技术和创新点

- 基于 Agent 提示固定性设计缓存策略，识别可重复利用的 KV 张量，减少冗余推理。
- 对传统 LRU 淘汰进行改进，引入工作流感知策略，避免缓存过早失效。
- **KV 预取机制**：后台线程在 GPU 闲置时，从 CPU 将未来步骤所需 KV 张量提前加载到 GPU，从而消除运行时停顿。
- 系统级框架 **KVFlow**：将缓存管理与预取机制整合，面向多 Agent 协同调用场景优化资源调度和执行顺序预测。
- 与现有解法（如 SGLang 的层级基数缓存）相比，KVFlow 更加适配多流程并发调用与大提示环境。

## 价值和展望

- **服务端效率提升**：对部署多 Agent LLM 的系统（如自动规划、对话协作等）大幅降低响应延迟与 GPU 占用。
- **扩展性强**：适用于多个不同 Agent 间协同执行的复杂任务调度场景。
- **未来方向**：可进一步结合智能调度策略预测 Agent 调用顺序，或扩展至异构硬件平台中的缓存迁移与管理。
- **商业价值**：助力云服务商、大型 LLM 应用平台提高并发处理能力，降低成本。

## 实验和效果

- 实验在单工作流、大提示长度场景下，KVFlow 达到 **约 1.83 ×** 的加速效果。
- 在并发调用多个工作流时，最高可实现 **约 2.19 ×** 加速，与 SGLang 的层级基数缓存对比。
- 实验还展示了预取机制如何减少 GPU 与 CPU 之间的数据交换停滞时间，提升整体推理吞吐率。
- 整体消除了缓存未命中引起的延迟峰值，提升系统稳定性和预测性。

## 总结

- KVFlow 针对 LLM 多 Agent 流程重用固定提示的特点，提出了 **工作流感知缓存与预取技术**，有效减少冗余计算和内存切换开销。
- 实验验证了其在单流程和并发流程场景中分别带来的显著加速（1.83×、2.19×）。
- KVFlow 提升了多 Agent 工作流程的整体效率，具备推广潜力，可为未来复杂、多任务 LLM 系统提供重要参考。

